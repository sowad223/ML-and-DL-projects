{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8201044,"sourceType":"datasetVersion","datasetId":4854718}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision open-clip-torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:32:09.051443Z","iopub.execute_input":"2024-12-30T19:32:09.051743Z","iopub.status.idle":"2024-12-30T19:32:13.778580Z","shell.execute_reply.started":"2024-12-30T19:32:09.051716Z","shell.execute_reply":"2024-12-30T19:32:13.777758Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\nCollecting open-clip-torch\n  Downloading open_clip_torch-2.29.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2024.9.11)\nCollecting ftfy (from open-clip-torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.66.5)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.24.7)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.4.5)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (1.0.12)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2024.8.30)\nDownloading open_clip_torch-2.29.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open-clip-torch\nSuccessfully installed ftfy-6.3.1 open-clip-torch-2.29.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom PIL import Image, ImageFile\n\n# Handle truncated images\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:35:57.687324Z","iopub.execute_input":"2024-12-30T19:35:57.687687Z","iopub.status.idle":"2024-12-30T19:35:57.692090Z","shell.execute_reply.started":"2024-12-30T19:35:57.687659Z","shell.execute_reply":"2024-12-30T19:35:57.691218Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train'\nval_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val'\ntest_dir = '/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test'\n\n# Preprocessing pipeline for CLIP\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to match CLIP input size\n    transforms.ToTensor(),         # Convert image to tensor\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize values\n])\n\n# Custom dataset to skip problematic images\nclass SafeImageFolder(ImageFolder):\n    def __getitem__(self, index):\n        try:\n            return super(SafeImageFolder, self).__getitem__(index)\n        except Exception as e:\n            print(f\"Error loading image at index {index}: {e}\")\n            return None\n\n# Data loaders with custom collate function to skip None\ndef custom_collate(batch):\n    return [b for b in batch if b is not None]\n\ntrain_dataset = SafeImageFolder(root=train_dir, transform=preprocess)\nval_dataset = SafeImageFolder(root=val_dir, transform=preprocess)\ntest_dataset = SafeImageFolder(root=test_dir, transform=preprocess)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate)\n\n# Verify dataset loading\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:37:33.089053Z","iopub.execute_input":"2024-12-30T19:37:33.089360Z","iopub.status.idle":"2024-12-30T19:37:36.997304Z","shell.execute_reply.started":"2024-12-30T19:37:33.089334Z","shell.execute_reply":"2024-12-30T19:37:36.996568Z"}},"outputs":[{"name":"stdout","text":"Training samples: 9246\nValidation samples: 829\nTest samples: 506\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import open_clip\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nclip_model, _, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\nclip_model.eval().to(device)  # Set CLIP to evaluation mode\n\n# Add classification head\nclass CLIPBinaryClassifier(nn.Module):\n    def __init__(self, clip_model, num_classes=2):\n        super(CLIPBinaryClassifier, self).__init__()\n        self.clip_model = clip_model\n        self.clip_model.eval()  # Freeze CLIP\n        self.classifier = nn.Linear(self.clip_model.visual.output_dim, num_classes)\n\n    def forward(self, images):\n        with torch.no_grad():  # Do not backpropagate through CLIP's encoder\n            features = self.clip_model.encode_image(images)\n        return self.classifier(features)\n\nmodel = CLIPBinaryClassifier(clip_model).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:03.237895Z","iopub.execute_input":"2024-12-30T19:38:03.238178Z","iopub.status.idle":"2024-12-30T19:38:05.018148Z","shell.execute_reply.started":"2024-12-30T19:38:03.238157Z","shell.execute_reply":"2024-12-30T19:38:05.017214Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=5):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in train_loader:\n            images, labels = zip(*batch)  # Unpack images and labels\n            images = torch.stack(images).to(device)\n            labels = torch.tensor(labels).to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n        validate_model(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:13.561496Z","iopub.execute_input":"2024-12-30T19:38:13.561778Z","iopub.status.idle":"2024-12-30T19:38:13.567057Z","shell.execute_reply.started":"2024-12-30T19:38:13.561756Z","shell.execute_reply":"2024-12-30T19:38:13.566078Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def validate_model(model, val_loader):\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            images, labels = zip(*batch)\n            images = torch.stack(images).to(device)\n            labels = torch.tensor(labels).to(device)\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n\n# Training the model\ntrain_model(model, train_loader, val_loader, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:38:27.677902Z","iopub.execute_input":"2024-12-30T19:38:27.678246Z","iopub.status.idle":"2024-12-30T19:49:57.732043Z","shell.execute_reply.started":"2024-12-30T19:38:27.678216Z","shell.execute_reply":"2024-12-30T19:49:57.731249Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.6727\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 68.76%\nEpoch [2/10], Loss: 0.6389\nValidation Accuracy: 71.89%\nEpoch [3/10], Loss: 0.6134\nValidation Accuracy: 75.27%\nEpoch [4/10], Loss: 0.5927\nValidation Accuracy: 76.24%\nEpoch [5/10], Loss: 0.5753\nValidation Accuracy: 75.03%\nEpoch [6/10], Loss: 0.5603\nValidation Accuracy: 77.68%\nEpoch [7/10], Loss: 0.5467\nValidation Accuracy: 77.68%\nEpoch [8/10], Loss: 0.5349\nValidation Accuracy: 77.93%\nEpoch [9/10], Loss: 0.5240\nValidation Accuracy: 77.20%\nEpoch [10/10], Loss: 0.5141\nValidation Accuracy: 77.32%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def test_model(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in test_loader:\n            images, labels = zip(*batch)\n            images = torch.stack(images).to(device)\n            labels = torch.tensor(labels).to(device)\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\n# Evaluate on the test dataset\ntest_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T19:53:25.852597Z","iopub.execute_input":"2024-12-30T19:53:25.852981Z","iopub.status.idle":"2024-12-30T19:53:36.768932Z","shell.execute_reply.started":"2024-12-30T19:53:25.852946Z","shell.execute_reply":"2024-12-30T19:53:36.768057Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 68.77%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}